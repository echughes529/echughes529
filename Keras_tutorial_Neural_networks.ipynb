{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/echughes529/echughes529/blob/main/Keras_tutorial_Neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W9p-Zi6VYOf"
      },
      "source": [
        "# **Neural networks**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In this tutorial, neural network is used to demonstrate how to use deep learning to predict the energy of a specific configuration of the water molecule. In Keras, there are a number of hyperparameters that can be customize and that affects the performance of your model significantly. We willl explore some of these hyperparameters in this tutorial.\n",
        "\n",
        "For this tutorial, you are provided with:\n",
        "1. ``H2O_unrotated.xyz`` and ``H2O_unrotated.ener``: a data set of 1750 configurations of the water molecule with random distortions. All molecules in this set are oriented in the same direction.\n",
        "2. ``H2O_rotated.xyz`` and ``H2O_rotated.ener``: a data set of 1750 configurations of the water molecule with random distortions, rotations, and translations. Molecules in this set are not oriented in the same direction.\n",
        "\n",
        "The goal is to have a machine learning model that is capable of predicting the energy of each configurations. This tutorial provides a guide for you to build your own model that can achieve this goal for both data sets.\n",
        "\n",
        "*(You can run this file to see what the commands do but please **do not** try to modify this file directly here. Your edits will not be saved. Make a copy in your Drive under the File tab in order to edit it.)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKuf3vh4Yz8i"
      },
      "source": [
        "## Building the model\n",
        "\n",
        "*(If you run this in Google Colab, you need to upload the data files to the content folder and modify the file path accordingly. If you run this on Jupyter Notebook, you need to put the data files and this code file in the same folder.)*\n",
        "\n",
        "Our model needs to take molecular structure information as input and predict the energy based on this information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aMnodfdDMf0a",
        "outputId": "50747006-764d-46de-97ee-e783d52140ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ase\n",
            "  Downloading ase-3.22.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 31.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from ase) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from ase) (1.21.6)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from ase) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.1.0->ase) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.1.0->ase) (1.15.0)\n",
            "Installing collected packages: ase\n",
            "Successfully installed ase-3.22.1\n"
          ]
        }
      ],
      "source": [
        "# import necessary libraries\n",
        "!pip install ase\n",
        "from ase.io import read\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path=\"/content/H2O_unrotated.xyz\"\n",
        "\n",
        "ftrs=read(file_path,':')\n",
        "nsample=len(ftrs)                                                   \n",
        "natom=[ftrs[i].get_number_of_atoms() for i in range(nsample)]\n",
        "coords=np.array([ftrs[i].get_positions() for i in range(nsample)])\n",
        "all_labels=[ftrs[i].get_chemical_symbols() for i in range(nsample)]\n",
        "\n",
        "\n",
        "# read the corresponding energy values\n",
        "ener=np.loadtxt(\"/content/H2O_unrotated.ener\")"
      ],
      "metadata": {
        "id": "78VdV8cXVuv5",
        "outputId": "a32e96cd-7c10-4d28-aa7c-46d47b62d7b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d1cdeb4797ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/H2O_unrotated.xyz\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mftrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mftrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnatom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mftrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_number_of_atoms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnsample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ase/io/formats.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(filename, index, format, parallel, do_not_split_by_at_sign, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m     \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfiletype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ioformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ase/io/formats.py\u001b[0m in \u001b[0;36mfiletype\u001b[0;34m(filename, read, guess)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morig_filename\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_with_compression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m             \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig_filename\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ase/io/formats.py\u001b[0m in \u001b[0;36mopen_with_compression\u001b[0;34m(filename, mode)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;31m# Either None or unknown string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/H2O_unrotated.xyz'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9SMBqSIaEpI"
      },
      "source": [
        "For neural network, as the method is numerical, we should normalize our input and output for better accuracy.\n",
        "$$\n",
        "x_i'=\\frac{x_i - \\bar{x}}{x_{max} - x_{min}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGEqup4ehkom"
      },
      "outputs": [],
      "source": [
        "#normalize inputs\n",
        "norm_coords=np.zeros((coords.shape))\n",
        "for a in range(natom[0]):\n",
        "  for j in range(3):\n",
        "    norm_coords[:,a,j] = (coords[:,a,j] - np.mean(coords[:,a,j]))/(np.max(coords[:,a,j]) - np.min(coords[:,a,j]))\n",
        "\n",
        "#normalize outputs\n",
        "norm_E = (ener - np.mean(ener))/(np.max(ener) - np.min(ener))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InVkyCbY8Tpj"
      },
      "source": [
        "First, we build a model with customizable parameters to make our codes short and neat. There are 2 basic ways to build a model in Keras, through the Sequential API and Functional API. Sequential models are straightforward and easy to create by building it layer by layer, but are not flexible (not allowing multiple inputs and outputs, no layer sharings, etc.) On the other hand, the Functional API is more flexible and powerful, allowing different customizations. In this tutorial, for the sake of simplicity, we will use the Sequential API and the `add()` method.\n",
        "\n",
        "You can test if the model architecture is as desired. In the table of this output, the layers are listed from input, hidden layers, and lastly, output layer. Each layer has its own output shape and the number of parameters that will be learned during the training process. The total number of parameters, trainable parameters, and non-trainable parameters are printed below the table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onCVXpuJbKTu"
      },
      "outputs": [],
      "source": [
        "# build and compile the model\n",
        "def build_compile_model(natom, reg, learn_rate):\n",
        "    model = keras.Sequential()\n",
        "    #specify an input layer with the number of nodes = the number of relative distances\n",
        "    #model.add(layers.Input(shape=(natom,3)))\n",
        "\n",
        "    #flatten the 2D input array into a 1D array, you can also do this with numpy .flatten()\n",
        "    model.add(layers.Flatten(input_shape=(natom,3)))\n",
        "\n",
        "    #add hidden layers\n",
        "    model.add(layers.Dense(32, activation='sigmoid', kernel_regularizer=keras.regularizers.l2(reg), bias_regularizer=keras.regularizers.l2(reg)))\n",
        "    model.add(layers.Dense(16, activation='sigmoid', kernel_regularizer=keras.regularizers.l2(reg), bias_regularizer=keras.regularizers.l2(reg)))\n",
        "    model.add(layers.Dense(8, activation='sigmoid', kernel_regularizer=keras.regularizers.l2(reg), bias_regularizer=keras.regularizers.l2(reg)))\n",
        "\n",
        "    #finally, add an output layer\n",
        "    model.add(layers.Dense(1, use_bias=False,kernel_regularizer=keras.regularizers.l2(reg)))\n",
        "\n",
        "    # define the optimizer: Adam is a gradient descent algorithm \n",
        "    opt=keras.optimizers.Adam(learning_rate=learn_rate)\n",
        "\n",
        "    #compile the model\n",
        "    model.compile(loss='mse',optimizer=opt)\n",
        "\n",
        "    return model\n",
        "\n",
        "model = build_compile_model(3, 1e-6, 0.001)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsgEVN0fy5Ez"
      },
      "source": [
        "The model is now ready to be trained. The data set is randomly divided here into the training set (with 1250 configurations), the validation set (with 250 configurations), and the test set (with 250 configurations). The validation set will mainly be used in the later sections to optimize the hyperparameters. The test set contains configurations your model has never seen and is used to test the generalizability of the model and its RMSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6rM7YwJSSJ-"
      },
      "outputs": [],
      "source": [
        "idx=np.arange(nsample)\n",
        "np.random.shuffle(idx)\n",
        "tr_coords=norm_coords[idx[:1250],:,:]\n",
        "tr_E=norm_E[idx[:1250]]\n",
        "va_coords=norm_coords[idx[1250:1500],:,:]\n",
        "va_E=norm_E[idx[1250:1500]]\n",
        "te_coords=norm_coords[idx[1500:],:,:]\n",
        "te_E=ener[idx[1500:]]\n",
        "\n",
        "history=model.fit(tr_coords,tr_E,epochs=500,validation_data=(va_coords,va_E),verbose=0)\n",
        "\n",
        "# plot a learning curve of loss of the training and validation set vs the number of epochs the model is trained\n",
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LcjVhL8M2Ag"
      },
      "outputs": [],
      "source": [
        "norm_predict=model.predict(te_coords)\n",
        "predict=norm_predict*(np.max(ener) - np.min(ener))+np.mean(ener)\n",
        "rmse = np.sqrt(np.mean((predict-te_E)**2))\n",
        "\n",
        "print(\"The model's RMSE is: \", rmse)\n",
        "#visualize the model's prediction vs the correct value\n",
        "plt.axes(aspect='equal')\n",
        "plt.scatter(te_E, predict)\n",
        "plt.xlabel('Reference Values (Hartree)')\n",
        "plt.ylabel('Predictions (Hartree)')\n",
        "plt.plot([np.min([np.min(te_E),np.min(predict)]),np.max([np.max(te_E),np.max(predict)])], [np.min([np.min(te_E),np.min(predict)]),np.max([np.max(te_E),np.max(predict)])], 'r-')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbbOal8Tc9Ky"
      },
      "source": [
        "## Effects of hyperparameters\n",
        "\n",
        "Hyperparameters usually affect the performance of your model significantly. Therefore, it is important to optimize them. Two hyperparameters are optimized in the following sections but (of course) there are many more that can be explored. You are encouraged to play with the values of the hyperparameters.\n",
        "\n",
        "1. Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exq8SdHadc1x"
      },
      "outputs": [],
      "source": [
        "regularization=[1,0.1,0.01,0.001,1e-4,1e-5,1e-6,1e-7,1e-8,1e-9]\n",
        "va_loss=[]\n",
        "for r in range(len(regularization)):\n",
        "  model = build_compile_model(3, regularization[r], 0.001)\n",
        "  history = model.fit(tr_coords,tr_E,epochs=500,validation_data=(va_coords,va_E),verbose=0)\n",
        "  va_predict=model.predict(va_coords)\n",
        "  # get the RMSE of the model prediction for the validation set\n",
        "  va_loss.append(np.sqrt(np.mean((va_predict-va_E)**2)))\n",
        "\n",
        "#visualize the effects of different regularization values on the model's accuracy\n",
        "plt.plot(regularization, va_loss, marker='o')\n",
        "plt.yscale(\"log\")\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel('Regularization value')\n",
        "plt.ylabel('Validation loss')\n",
        "plt.show()\n",
        "\n",
        "# get the regularization value corresponding to the smallest validation error\n",
        "for i in range(len(va_loss)):\n",
        "    if va_loss[i]==min(va_loss):\n",
        "        reg=regularization[i]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1FaaFn2ddia"
      },
      "source": [
        "2. Learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ha07dMgdf3c"
      },
      "outputs": [],
      "source": [
        "learning_rate=[1,0.1,0.01,0.001,0.0001,1e-5]\n",
        "va_loss=[]\n",
        "for lr in range(len(learning_rate)):\n",
        "  model = build_compile_model(3, reg, learning_rate[lr])\n",
        "  history = model.fit(tr_coords,tr_E,epochs=500,validation_data=(va_coords,va_E),verbose=0)\n",
        "  va_predict=model.predict(va_coords)\n",
        "  # get the RMSE of the model prediction for the validation set\n",
        "  va_loss.append(np.sqrt(np.mean((va_predict-va_E)**2)))\n",
        "\n",
        "#visualize the effects of different learning rate values on the model's accuracy\n",
        "plt.plot(learning_rate, va_loss, marker='o')\n",
        "plt.yscale(\"log\")\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel('Learning rate value')\n",
        "plt.ylabel('Validation loss')\n",
        "plt.show()\n",
        "\n",
        "# get the learning rate value corresponding to the smallest validation error\n",
        "for i in range(len(va_loss)):\n",
        "    if va_loss[i]==min(va_loss):\n",
        "        learn_rate=learning_rate[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sA4wflhdjMu"
      },
      "source": [
        "## Learning curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdZSaq8MH8bs"
      },
      "source": [
        "It would be interesting to show how the model would behave if trained on a smaller or larger data set. It is a way to tell if you need to gather more data to improve the model's accuracy or if it's saturated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXj5eHpqdl-6"
      },
      "outputs": [],
      "source": [
        "training_points=[1500,1250,1000,750,500,250,100,50]\n",
        "te_rmse=[]\n",
        "for p in range(len(training_points)):\n",
        "  ntrain=training_points[p]\n",
        "  tr_coords=norm_coords[idx[:ntrain],:]\n",
        "  tr_E=norm_E[idx[:ntrain]]\n",
        "\n",
        "  model.fit(tr_coords,tr_E,epochs=1000,validation_data=(va_coords,va_E),verbose=0)\n",
        "\n",
        "  norm_predict=model.predict(te_coords)\n",
        "  predict=norm_predict*(np.max(ener) - np.min(ener))+np.mean(ener)\n",
        "  te_rmse.append(np.sqrt(np.mean((predict-te_E)**2)))\n",
        "\n",
        "plt.plot(training_points, te_rmse, marker='o')\n",
        "plt.xlabel('Training points')\n",
        "plt.ylabel('Model RMSE (Hartree)')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}